{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyapacheatlas.auth import ServicePrincipalAuthentication\n",
    "from pyapacheatlas.core import PurviewClient\n",
    "from pyapacheatlas.core.util import GuidTracker\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "tenant_id = os.environ.get(\"TENANT_ID\") \n",
    "client_id = os.environ.get(\"CLIENT_ID\")\n",
    "client_secret = os.environ.get(\"CLIENT_SECRET\")\n",
    "account_name = os.environ.get(\"PURVIEW_ACCOUNT\")\n",
    "\n",
    "auth = ServicePrincipalAuthentication(\n",
    "    tenant_id = tenant_id, \n",
    "    client_id = client_id, \n",
    "    client_secret = client_secret\n",
    ")\n",
    "\n",
    "# Create a client to connect to your service.\n",
    "client = PurviewClient(\n",
    "    account_name = account_name,\n",
    "    authentication = auth\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_all_typedefs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.endpoint_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_entity(qualifiedName=\"https://purviewatlaspoc.dfs.core.windows.net/datalake/extended_vaccine_data/{SparkPartitions}\", typeName=\"azure_datalake_gen2_resource_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyapacheatlas.core import AtlasClient, AtlasEntity, TypeCategory, AtlasProcess\n",
    "\n",
    "# Create a Tabular Schema entity\n",
    "ts = AtlasEntity(\n",
    "        name=\"demoSchema\",\n",
    "        typeName=\"tabular_schema\",\n",
    "        qualified_name=\"pyapache://demotabschema\",\n",
    "        guid = -1\n",
    "    )\n",
    "    # Create a Column entity that references your tabular schema\n",
    "col01 = AtlasEntity(\n",
    "        name=\"demoColumn\",\n",
    "        typeName=\"column\",\n",
    "        qualified_name=\"pyapache://mycolumn\",\n",
    "        guid= -2,\n",
    "        attributes={\n",
    "            \"type\":\"String\",\n",
    "            \"description\": \"This is an example column\"\n",
    "        },\n",
    "        relationshipAttributes = {\n",
    "            \"composeSchema\": ts.to_json(minimum=True)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create a resource set that references the tabular schema\n",
    "rs = AtlasEntity(\n",
    "        name=\"demoresourceset\",\n",
    "        typeName=\"azure_datalake_gen2_resource_set\",\n",
    "        qualified_name=\"pyapache://demors\",\n",
    "        guid = -3,\n",
    "        relationshipAttributes = {\n",
    "            \"tabular_schema\": ts.to_json(minimum=True)\n",
    "        }\n",
    "    )\n",
    "\n",
    "client.upload_entities([ts, rs, col01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get datasources\n",
    "\n",
    "import requests\n",
    "\n",
    "client.scan_endpoint_url = \"https://purviewatlaspoc.scan.purview.azure.com\"\n",
    "url = client.scan_endpoint_url + \\\n",
    "    f\"/datasources\"\n",
    "\n",
    "getDataSources = requests.get(\n",
    "    url,\n",
    "    headers =  client.authentication.get_authentication_headers(),\n",
    "    params = { \"api-version\": \"2018-12-01-preview\"}\n",
    ")\n",
    "\n",
    "response = client._handle_response(getDataSources)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create or update scan\n",
    "import requests\n",
    "\n",
    "scan_name = \"datalakeScan\"\n",
    "data_source = \"AzureDataLakeStorage-ZSB\"\n",
    "\n",
    "client.scan_endpoint_url = \"https://purviewatlaspoc.scan.purview.azure.com\"\n",
    "url = client.scan_endpoint_url + \\\n",
    "    f\"/datasources/{data_source}/scans/{scan_name}\"\n",
    "\n",
    "createOrUpdateScan = requests.put(\n",
    "    url,\n",
    "    headers =  client.authentication.get_authentication_headers(),\n",
    "    params = { \"api-version\": \"2018-12-01-preview\"},\n",
    "    json = {\n",
    "    \"kind\": \"AdlsGen2Msi\",\n",
    "    \"properties\": {\n",
    "        \"scanRulesetName\": \"AdlsGen2\",\n",
    "        \"scanRulesetType\": \"System\",\n",
    "        \"collection\": {\n",
    "            \"referenceName\": account_name,\n",
    "            \"type\": \"CollectionReference\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    ")\n",
    "\n",
    "response = client._handle_response(createOrUpdateScan)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(*columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_df(df: DataFrame, name: str, qualified_name: str):\n",
    "    colEntities = []\n",
    "    guid = 100\n",
    "\n",
    "    ts = AtlasEntity(\n",
    "            name=\"demoDFSchema\",\n",
    "            typeName=\"tabular_schema\",\n",
    "            qualified_name=f\"{qualified_name}_tabular_schema\",\n",
    "            guid = -guid\n",
    "        )\n",
    "\n",
    "    for (col, type) in df.dtypes:\n",
    "        guid +=1\n",
    "        colEntities.append(\n",
    "            AtlasEntity(\n",
    "                name=col,\n",
    "                typeName=\"column\",\n",
    "                qualified_name=f\"{qualified_name}_column_{col}\",\n",
    "                guid= -guid,\n",
    "                attributes={\n",
    "                    \"type\": type,\n",
    "                    \"description\": f\"Column {col} has type {type}\"\n",
    "                },\n",
    "                relationshipAttributes = {\n",
    "                    \"composeSchema\": ts.to_json(minimum=True)\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    rs = AtlasEntity(\n",
    "            name=name,\n",
    "            typeName=\"azure_datalake_gen2_resource_set\",\n",
    "            qualified_name=qualified_name,\n",
    "            guid = -(guid+1),\n",
    "            relationshipAttributes = {\n",
    "                \"tabular_schema\": ts.to_json(minimum=True)\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return (rs, client.upload_entities([ts, rs, *colEntities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs, response = register_df(df=df, name=\"testDF\", qualified_name=\"pyapache://testDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root directory to path so that python can import src.utils. Alternatively, turn into installable python package.\n",
    "import sys  \n",
    "sys.path.insert(0, os.getcwd().rsplit('/',1)[0])\n",
    "\n",
    "from src.utils import PurviewPOCClient\n",
    "\n",
    "client = PurviewPOCClient(account_name=account_name, authentication=auth)\n",
    "client.create_delta_table_typedefs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import PurviewPOCClient\n",
    "\n",
    "client = PurviewPOCClient(account_name=account_name, authentication=auth)\n",
    "\n",
    "r1, t1, cols1 = client.register_df(df, \"input1\", \"pyapache://input1DF\")\n",
    "r2, t2, cols2 = client.register_df(df.alias(\"input2\"), \"input2\", \"pyapache://input2DF\")\n",
    "r3, t3, cols3 = client.register_df(df.alias(\"output1\"), \"output1\", \"pyapache://output1DF\")\n",
    "\n",
    "process = AtlasProcess(\n",
    "  name=\"test_spark_job\",\n",
    "  qualified_name = \"pyapacheatlas://test_spark_job\",\n",
    "  typeName=\"custom_spark_job_process\",\n",
    "  guid=-20,\n",
    "  attributes = {\"job_type\":\"join\"},\n",
    "  inputs = [r1, r2],\n",
    "  outputs = [r3]\n",
    ")\n",
    "\n",
    "client.upload_entities([r1, t1, *cols1, r2, t2, *cols2, r3, t3, *cols3, process])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12cd73f1a6436e8c8f62fb5a4710859da72797e342b0cc814b066833968a3542"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
